{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.640s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestStringMethods(unittest.TestCase):\n",
    "\n",
    "    def test_userprofile(self):\n",
    "        \n",
    "        def get_desc(desc_set, colLength, count):\n",
    "            count = desc_set.shape[1]\n",
    "            while(count < colLength):\n",
    "                desc_set = np.append(desc_set, 1)\n",
    "                count += 1\n",
    "            \n",
    "            return desc_set\n",
    "        \n",
    "        # Source: https://stackoverflow.com/questions/1518522/python-most-common-element-in-a-list\n",
    "        # by user Alex Martelli\n",
    "        def most_common(L):\n",
    "            # get an iterable of (item, iterable) pairs\n",
    "            SL = sorted((x, i) for i, x in enumerate(L))\n",
    "            # print 'SL:', SL\n",
    "            groups = itertools.groupby(SL, key=operator.itemgetter(0))\n",
    "            # auxiliary function to get \"quality\" for an item\n",
    "            def _auxfun(g):\n",
    "                item, iterable = g\n",
    "                count = 0\n",
    "                min_index = len(L)\n",
    "                for _, where in iterable:\n",
    "                    count += 1\n",
    "                min_index = min(min_index, where)\n",
    "                # print 'item %r, count %r, minind %r' % (item, count, min_index)\n",
    "                return count, -min_index\n",
    "            # pick the highest-count/earliest item\n",
    "            result = max(groups, key=_auxfun)[0]\n",
    "            return result\n",
    "    \n",
    "        # Source: https://stackoverflow.com/questions/32037893/numpy-fix-array-with-rows-of-different-lengths-by-filling-the-empty-elements-wi\n",
    "        # by user Divakar\n",
    "        def numpy_fillna(data):\n",
    "            # Get lengths of each row of data\n",
    "            lens = np.array([len(i) for i in data])\n",
    "    \n",
    "            # Mask of valid places in each row\n",
    "            mask = np.arange(lens.max()) < lens[:,None]\n",
    "    \n",
    "            # Setup output array and put elements from data into masked positions\n",
    "            out = np.zeros(mask.shape, dtype=data.dtype)\n",
    "            out[mask] = np.concatenate(data)\n",
    "            return out    \n",
    "    \n",
    "        def tokenize(s):\n",
    "            return tokens_re.findall(s)\n",
    "    \n",
    "            #args s = tweets, lowercase=False(keeps as in tweets) / True(changes all to lowercase)\n",
    "        def preprocess(s, lowercase=True):\n",
    "            tokens = tokenize(s)\n",
    "            if lowercase:\n",
    "                tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "            return tokens\n",
    "        \n",
    "        from sklearn.svm import LinearSVC\n",
    "        from nltk.corpus import stopwords\n",
    "        import pickle\n",
    "        import re\n",
    "        import string\n",
    "\n",
    "        import numpy as np\n",
    "        import tweepy\n",
    "        import operator\n",
    "        import itertools\n",
    "        import warnings\n",
    "        \n",
    "        warnings.filterwarnings('ignore')\n",
    "        \n",
    "        #Auth details from Twitter app http://twitclassify.com/\n",
    "        CONSUMER_KEY = \"90bt0hlKzf3zWi5aPJ5ELZOGL\"\n",
    "        CONSUMER_SECRET = \"H4ycskJVRjcesIaZBVUFdTClsucdMEyPcZyQT7Xjvou3LWwBg2\"\n",
    "        ACCESS_KEY = \"303590178-hw3mgzNW08xAvbrVW4Zm8VP4uKpU9qyfG01ynkzi\"\n",
    "        ACCESS_SECRET = \"6wJw3hGpFAgR77nk6Hw4UjCGi96OtOZhnXjm1iMcptQ3i\"\n",
    "        \n",
    "        authid = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "        authid.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "        myApi = tweepy.API(authid, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "        \n",
    "        punctuation = list(string.punctuation)\n",
    "        stop = stopwords.words('english') + punctuation + ['rt', 'via', 'if', 'i', '’', '“', '”', 'it', 'the', '…', 'us', ':)', '—',\n",
    "                                                           ':/', 'il', '', '≠', '•']\n",
    "        \n",
    "        # Source taken from https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/\n",
    "        emoticons_str = r\"\"\"\n",
    "            (?:\n",
    "                [:=;] # Eyes\n",
    "                [oO\\-]? # Nose (optional)\n",
    "                [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "            )\"\"\"\n",
    "         \n",
    "        regex_str = [\n",
    "            emoticons_str,\n",
    "            r'<[^>]+>', # HTML tags\n",
    "            r'(?:@[\\w_]+)', # @-mentions\n",
    "            r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "            r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "         \n",
    "            r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "            r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "            r'(?:[\\w_]+)', # other words\n",
    "            r'(?:\\S)' # anything else\n",
    "        ]\n",
    "            \n",
    "        tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "        emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "           \n",
    "        ### Lists ###\n",
    "        desc_set = []\n",
    "        li = []\n",
    "        convList = []\n",
    "        \n",
    "        twitHandle = ['sdrogers']\n",
    " \n",
    "        user_objects = myApi.lookup_users(screen_names= twitHandle)\n",
    "        user_id = [user.id_str for user in user_objects]\n",
    "        userProfile = myApi.lookup_users(user_ids=user_id)\n",
    "        \n",
    "        for user in userProfile:\n",
    "           \n",
    "            #user profile description\n",
    "            terms_stop = [term for term in preprocess(user.description) \n",
    "                          if term not in stop and\n",
    "                          not term.startswith(('#', '@', 'http', 'https'))]\n",
    "            \n",
    "            # separates each descriptions to their respective users\n",
    "            # add list of desc for each user into desc train set ie. desc_train_set = [[desc 1][desc 2]...[desc n]]\n",
    "            desc_set.append(terms_stop)\n",
    "        \n",
    "        #load pickle profileDict here\n",
    "        with open('profileDict.pkl', 'rb') as f:\n",
    "            uniqueDict = pickle.load(f)\n",
    "            \n",
    "        #inv_map = {v: k for k, v in uniqueDict.items()} # to reverse dict ordering\n",
    "        for index,word in enumerate(desc_set):         # iterate over set    \n",
    "            for key, value in uniqueDict.items():\n",
    "                if value in word:                      # see if word exists in dictionary\n",
    "                    convList.append(key)\n",
    "            li.append(convList)\n",
    "            convList = []\n",
    "    \n",
    "        desc_set = np.asarray(li)\n",
    "        # Match dimensions of array with highest no. of feature\n",
    "        desc_set = numpy_fillna(desc_set)\n",
    "    \n",
    "        with open('profileColLen.pkl', 'rb') as f:\n",
    "            colLength = pickle.load(f)\n",
    "        count = int(desc_set.shape[1])\n",
    "        \n",
    "        desc = get_desc(desc_set, colLength, count)\n",
    "        # Load from .pickle\n",
    "        with open('profileModel.pkl','rb') as f:\n",
    "            modelSVM = pickle.load(f)\n",
    "            \n",
    "            # make predictions\n",
    "        predicted = modelSVM.predict(desc)\n",
    "        # summarize the fit of the model\n",
    "        # Classification report\n",
    "        num = most_common(predicted)\n",
    "    \n",
    "        if(num == '1'):\n",
    "            community = \"Academic\"\n",
    "        elif(num == '2'):\n",
    "            community = \"Media\"\n",
    "        elif(num == '3'):\n",
    "            community = \"Business\"\n",
    "        elif(num == '4'):\n",
    "            community = \"Politic\"\n",
    "        elif(num == '5'):\n",
    "            community = \"Citizen\"\n",
    "    \n",
    "        category = community        \n",
    "                \n",
    "        self.assertEqual(category, 'Academic')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
