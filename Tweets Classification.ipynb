{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter User Community Classification ( Tweets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112\n",
      "Finished creating dictionary!\n",
      "Done conversion!\n",
      "[ [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      " [22, 31, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]\n",
      " [5, 67, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110]\n",
      " [15, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152]\n",
      " [10, 18, 22, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175]\n",
      " [5, 94, 176, 177, 178, 179, 180, 181, 182, 183]\n",
      " [4, 150, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235]\n",
      " [37, 132, 154, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278]\n",
      " [22, 25, 67, 197, 232, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301]\n",
      " [10, 22, 159, 165, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325]\n",
      " [15, 39, 56, 160, 243, 247, 317, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355]\n",
      " [78, 138, 139, 176, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379]\n",
      " [9, 10, 71, 177, 242, 316, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390]\n",
      " [5, 9, 10, 18, 39, 102, 138, 153, 336, 358, 370, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415]\n",
      " [32, 33, 88, 138, 148, 150, 158, 159, 243, 303, 342, 367, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437]\n",
      " [4, 10, 34, 154, 358, 390, 430, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457]\n",
      " [10, 15, 148, 180, 219, 323, 336, 390, 391, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470]\n",
      " [4, 42, 74, 154, 341, 395, 457, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506]\n",
      " [31, 56, 87, 284, 361, 453, 475, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540]\n",
      " [5, 25, 39, 42, 138, 153, 154, 320, 323, 396, 440, 475, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565]\n",
      " [4, 5, 22, 102, 107, 138, 162, 166, 360, 462, 525, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597]\n",
      " [5, 12, 31, 37, 39, 58, 102, 204, 358, 499, 525, 561, 563, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625]\n",
      " [18, 22, 42, 67, 93, 101, 138, 166, 203, 274, 282, 333, 516, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654]\n",
      " [5, 10, 56, 120, 270, 298, 520, 578, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692]\n",
      " [20, 148, 378, 508, 510, 693, 694, 695, 696, 697, 698, 699, 700, 701]\n",
      " [25, 54, 135, 167, 323, 339, 408, 409, 460, 492, 682, 691, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727]\n",
      " [15, 20, 44, 315, 351, 649, 661, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752]\n",
      " [4, 10, 56, 61, 102, 269, 273, 395, 433, 446, 475, 549, 694, 743, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775]\n",
      " [18, 71, 87, 89, 148, 312, 709, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792]\n",
      " [175, 178, 198, 439, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829]]\n",
      "(30,)\n",
      "(30, 54)\n"
     ]
    }
   ],
   "source": [
    "def find_bigrams(input_list):\n",
    "    return zip(input_list, input_list[1:])\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/32037893/numpy-fix-array-with-rows-of-different-lengths-by-filling-the-empty-elements-wi\n",
    "# by user Divakar\n",
    "def numpy_fillna(data):\n",
    "    # Get lengths of each row of data\n",
    "    lens = np.array([len(i) for i in data])\n",
    "\n",
    "    # Mask of valid places in each row\n",
    "    mask = np.arange(lens.max()) < lens[:,None]\n",
    "\n",
    "    # Setup output array and put elements from data into masked positions\n",
    "    out = np.zeros(mask.shape, dtype=data.dtype)\n",
    "    out[mask] = np.concatenate(data)\n",
    "    return out\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "    #args s = tweets, lowercase=False(keeps as in tweets) / True(changes all to lowercase)\n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "import tweepy\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from itertools import chain\n",
    "from scipy import sparse\n",
    "from IPython import display\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "\n",
    "#Auth details from Twitter app http://twitclassify.com/\n",
    "CONSUMER_KEY = \"90bt0hlKzf3zWi5aPJ5ELZOGL\"\n",
    "CONSUMER_SECRET = \"H4ycskJVRjcesIaZBVUFdTClsucdMEyPcZyQT7Xjvou3LWwBg2\"\n",
    "ACCESS_KEY = \"303590178-hw3mgzNW08xAvbrVW4Zm8VP4uKpU9qyfG01ynkzi\"\n",
    "ACCESS_SECRET = \"6wJw3hGpFAgR77nk6Hw4UjCGi96OtOZhnXjm1iMcptQ3i\"\n",
    "\n",
    "authid = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "authid.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "myApi = tweepy.API(authid, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', 'if', 'i', '’', '“', '”', 'it', 'the', '…', 'us', ':)', '—',\n",
    "                                                   ':/', 'il', '', '≠', '•']\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "# Source taken from https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "data_train = np.genfromtxt(\"trainingfriends.tsv\", delimiter='\\t', dtype= str)\n",
    "\n",
    "twitHandle = data_train[:,1] # this gets the tweet handles [column B]\n",
    "y = data_train[:,0]\n",
    "\n",
    "train_set = []\n",
    "bigrams = []\n",
    "li = []\n",
    "user_ids = []\n",
    "uniqueWords = []\n",
    "uniqueDict= {}\n",
    "convList = []\n",
    "bigList = []\n",
    "\n",
    "print(\"Getting users...\")\n",
    "for i in range(len(twitHandle)):\n",
    "    #get user's timeline of recent tweets on # of items\\n\",\n",
    "\n",
    "    display.clear_output(wait=True) #refresh display\n",
    "    print(i+1) # Show progress of user retrieval\n",
    "    \n",
    "    status = tweepy.Cursor(myApi.user_timeline, screen_name = twitHandle[i]).items(5)\n",
    "    while True:\n",
    "        try:\n",
    "            tweet = status.next()\n",
    "            \n",
    "            # Count terms only (no hashtags, no mentions)\\n\",\n",
    "            terms_only = [term for term in preprocess(tweet.text)\n",
    "                      if term not in stop and\n",
    "                      not term.startswith(('#', '@', 'http', 'https'))]\n",
    "\n",
    "            li = li + terms_only\n",
    "            \n",
    "        except tweepy.TweepError: \n",
    "            time.sleep(60 * 15)\n",
    "            continue\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    # Append all unique unigram terms into uniqeWords list\n",
    "    for j in range(len(li)):\n",
    "        for word in li:\n",
    "            if not word in uniqueWords:\n",
    "                uniqueWords.append(word)\n",
    "\n",
    "    train_set.append(li)\n",
    "    \n",
    "    # empty the list for next iteration\n",
    "    li = []\n",
    "\n",
    "#print(train_set) # [[1][2][n]] should be 1 .. len(twitHandle)\n",
    "print(\"Finished getting users!\")\n",
    "\n",
    "bigrams = list(chain.from_iterable(train_set))\n",
    "bigrams = list(find_bigrams(bigrams))\n",
    "\n",
    "# Append all unique bigram terms into uniqeWords list\n",
    "for i in range(len(bigrams)):\n",
    "    display.clear_output(wait=True) #refresh display\n",
    "    print(i)\n",
    "    for word in bigrams:\n",
    "        if not word in uniqueWords:\n",
    "            uniqueWords.append(word)\n",
    "\n",
    "# Initializes a key, value of (eg. {term: id} ) into dictionary\n",
    "for i,j in enumerate(uniqueWords):\n",
    "    uniqueDict[i+1] = (j,i)\n",
    "\n",
    "for j in range(len(uniqueDict)): # to create a single dictionary\n",
    "    uniqueDict[j+1] = uniqueDict[j+1][0]\n",
    "\n",
    "print(\"Finished creating dictionary!\")\n",
    "\n",
    "# Save to .pickle\n",
    "with open('saveDict.pkl', 'wb') as f:\n",
    "    pickle.dump(uniqueDict, f)\n",
    "\n",
    "# Load from .pickle\n",
    "with open('saveDict.pkl','rb') as f:\n",
    "    uniqueDict = pickle.load(f)\n",
    "\n",
    "inv_map = {v: k for k, v in uniqueDict.items()} # to reverse dict ordering\n",
    "for index,word in enumerate(train_set):         # iterate over set\n",
    "    for key, value in uniqueDict.items():\n",
    "        if value in word:                      # see if word exists in dictionary\n",
    "            convList.append(key)\n",
    "    bigList.append(convList)\n",
    "    convList = []\n",
    "\n",
    "print(\"Done conversion!\")\n",
    "\n",
    "train_set = np.asarray(bigList)\n",
    "\n",
    "print(train_set)\n",
    "\n",
    "print(train_set.shape)\n",
    "train_set = numpy_fillna(train_set) # Match dimensions of array with highest no. of feature\n",
    "print(train_set.shape)\n",
    "\n",
    "#splits data into training and test set\n",
    "trainX, testX, train_y, test_y = train_test_split(train_set, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=5.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Community:  Citizen\n",
      "Classification report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.33      0.50      0.40         2\n",
      "          2       0.67      0.50      0.57         4\n",
      "          3       0.00      0.00      0.00         4\n",
      "          4       0.00      0.00      0.00         2\n",
      "          5       0.20      0.33      0.25         3\n",
      "\n",
      "avg / total       0.26      0.27      0.26        15\n",
      "\n",
      "Confusion Metric = \n",
      " [[1 1 0 0 0]\n",
      " [1 2 0 0 1]\n",
      " [0 0 0 2 2]\n",
      " [0 0 1 0 1]\n",
      " [1 0 0 1 1]]\n",
      "Accuracy =  0.266666666667\n",
      "Sensitivity =  0.5\n",
      "Specificity =  0.666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/1518522/python-most-common-element-in-a-list\n",
    "# by user Alex Martelli\n",
    "def most_common(L):\n",
    "    # get an iterable of (item, iterable) pairs\n",
    "    SL = sorted((x, i) for i, x in enumerate(L))\n",
    "    # print 'SL:', SL\n",
    "    groups = itertools.groupby(SL, key=operator.itemgetter(0))\n",
    "    # auxiliary function to get \"quality\" for an item\n",
    "    def _auxfun(g):\n",
    "        item, iterable = g\n",
    "        count = 0\n",
    "        min_index = len(L)\n",
    "        for _, where in iterable:\n",
    "            count += 1\n",
    "            min_index = min(min_index, where)\n",
    "        # print 'item %r, count %r, minind %r' % (item, count, min_index)\n",
    "        return count, -min_index\n",
    "    # pick the highest-count/earliest item\n",
    "    result = max(groups, key=_auxfun)[0]\n",
    "    return result\n",
    "\n",
    "# fit a SVM model to the data\n",
    "modelSVM = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, tol=0.0001, C=5.0, multi_class='ovr',\n",
    "                     fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0,\n",
    "                     random_state=None, max_iter=1000)\n",
    "modelSVM.fit(trainX, train_y)\n",
    "print(modelSVM)\n",
    "# make predictions\n",
    "expected = test_y\n",
    "predicted = modelSVM.predict(testX)\n",
    "# summarize the fit of the model\n",
    "# Classification report\n",
    "num = most_common(predicted)\n",
    "\n",
    "if(num == '1'):\n",
    "    community = \"Academic\"\n",
    "elif(num == '2'):\n",
    "    community = \"Media\"\n",
    "elif(num == '3'):\n",
    "    community = \"Business\"\n",
    "elif(num == '4'):\n",
    "    community = \"Politic\"\n",
    "elif(num == '5'):\n",
    "    community = \"Citizen\"\n",
    "\n",
    "print(\"Community: \", community)\n",
    "class_report = metrics.classification_report(expected, predicted)\n",
    "print(\"Classification report \\n\", class_report)\n",
    "\n",
    "# Confusion Metric\n",
    "confMatrix = metrics.confusion_matrix(expected, predicted)\n",
    "print(\"Confusion Metric = \\n\", confMatrix)\n",
    "\n",
    "# Accuracy Score\n",
    "print(\"Accuracy = \", metrics.accuracy_score(expected, predicted))\n",
    "\n",
    "#True Positive Rate\n",
    "tpr = (confMatrix[0,0] / (confMatrix[1,0] + confMatrix[0,0]))\n",
    "print(\"Sensitivity = \", tpr)\n",
    "\n",
    "#True Negative Rate\n",
    "tnr = (confMatrix[1,1] / (confMatrix[0,1] + confMatrix[1,1]))\n",
    "print(\"Specificity = \", tnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "          1       0.00      0.00      0.00         2\n",
      "          2       0.00      0.00      0.00         4\n",
      "          3       0.00      0.00      0.00         4\n",
      "          4       1.00      0.50      0.67         2\n",
      "          5       0.11      0.33      0.17         3\n",
      "\n",
      "avg / total       0.16      0.13      0.12        15\n",
      "\n",
      "Confusion Metric = \n",
      " [[0 1 0 0 1]\n",
      " [1 0 0 0 3]\n",
      " [0 1 0 0 3]\n",
      " [0 0 0 1 1]\n",
      " [0 1 1 0 1]]\n",
      "Accuracy =  0.133333333333\n",
      "Sensitivity =  0.0\n",
      "Specificity =  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Multinomial Naive Bayes Classifier full data\n",
    "\n",
    "# fit a SVM model to the data\n",
    "modelMNB = MultinomialNB()\n",
    "modelMNB.fit(trainX, train_y)\n",
    "print(modelMNB)\n",
    "# make predictions\n",
    "expected = test_y\n",
    "predicted = modelMNB.predict(testX)\n",
    "# summarize the fit of the model\n",
    "# Classification report\n",
    "print(\"Classification report \", metrics.classification_report(expected, predicted))\n",
    "\n",
    "# Confusion Metric\n",
    "confMatrix = metrics.confusion_matrix(expected, predicted)\n",
    "print(\"Confusion Metric = \\n\", confMatrix)\n",
    "\n",
    "# Accuracy Score\n",
    "print(\"Accuracy = \", metrics.accuracy_score(expected, predicted))\n",
    "\n",
    "#True Positive Rate\n",
    "tpr = (confMatrix[0,0] / (confMatrix[1,0] + confMatrix[0,0]))\n",
    "print(\"Sensitivity = \", tpr)\n",
    "\n",
    "#True Negative Rate\n",
    "tnr = (confMatrix[1,1] / (confMatrix[0,1] + confMatrix[1,1]))\n",
    "print(\"Specificity = \", tnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "          1       0.33      0.50      0.40         2\n",
      "          2       0.40      0.50      0.44         4\n",
      "          3       0.00      0.00      0.00         4\n",
      "          4       0.00      0.00      0.00         2\n",
      "          5       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.15      0.20      0.17        15\n",
      "\n",
      "Confusion Metric = \n",
      " [[1 1 0 0 0]\n",
      " [1 2 0 0 1]\n",
      " [0 1 0 1 2]\n",
      " [0 0 1 0 1]\n",
      " [1 1 0 1 0]]\n",
      "Accuracy =  0.2\n",
      "Sensitivity =  0.5\n",
      "Specificity =  0.666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit a Logistic Regression model to the data\n",
    "modelLR = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True,\n",
    "                   intercept_scaling=1, class_weight=None, random_state=None,\n",
    "                   solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "modelLR.fit(trainX, train_y)\n",
    "# make predictions\n",
    "expected = test_y\n",
    "predicted = modelLR.predict(testX)\n",
    "# summarize the fit of the model\n",
    "# Classification report\n",
    "print(\"Classification report \", metrics.classification_report(expected, predicted))\n",
    "\n",
    "# Confusion Metric\n",
    "confMatrix = metrics.confusion_matrix(expected, predicted)\n",
    "print(\"Confusion Metric = \\n\", confMatrix)\n",
    "\n",
    "# Accuracy Score\n",
    "print(\"Accuracy = \", metrics.accuracy_score(expected, predicted))\n",
    "\n",
    "#True Positive Rate\n",
    "tpr = (confMatrix[0,0] / (confMatrix[1,0] + confMatrix[0,0]))\n",
    "print(\"Sensitivity = \", tpr)\n",
    "\n",
    "#True Negative Rate\n",
    "tnr = (confMatrix[1,1] / (confMatrix[0,1] + confMatrix[1,1]))\n",
    "print(\"Specificity = \", tnr)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
