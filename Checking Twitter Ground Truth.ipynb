{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def lookupUserById():\n",
    "    userProfile = myApi.lookup_users(user_ids=['813286'])\n",
    "    \n",
    "    for user in userProfile:\n",
    "        print (\"Twitter Handle = \", user.screen_name)\n",
    "        print (\"Username = \",user.name)\n",
    "        print (\"Description = \",user.description)\n",
    "        print (\"User ID = \", user.id)\n",
    "        print (\"Total Followers = \",user.followers_count)\n",
    "        print (\"Total Tweets = \",user.statuses_count)\n",
    "        print (\"Web URL = \",user.url, '\\n')\n",
    "        \n",
    "        #user profile description\n",
    "        terms_stop = [term for term in preprocess(user.description) if term not in stop]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_stop)\n",
    "\n",
    "        # Print the first N most frequent words\n",
    "        print(\"Frequent words: \", count_all.most_common(20))\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "    #args s = tweets, lowercase=False(keeps as in tweets) / True(changes all to lowercase)\n",
    "def preprocess(s, lowercase=True):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "#Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython import display\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "\n",
    "\n",
    "#Auth details from Twitter app http://twitclassify.com/\n",
    "CONSUMER_KEY = \"90bt0hlKzf3zWi5aPJ5ELZOGL\"\n",
    "CONSUMER_SECRET = \"H4ycskJVRjcesIaZBVUFdTClsucdMEyPcZyQT7Xjvou3LWwBg2\"\n",
    "ACCESS_KEY = \"303590178-hw3mgzNW08xAvbrVW4Zm8VP4uKpU9qyfG01ynkzi\"\n",
    "ACCESS_SECRET = \"6wJw3hGpFAgR77nk6Hw4UjCGi96OtOZhnXjm1iMcptQ3i\"\n",
    "\n",
    "authid = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "authid.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "myApi = tweepy.API(authid)\n",
    "\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', 'if', 'i', '’', '“', '”', 'it', 'the', '…', 'us', ':)', '—',\n",
    "                                                  ':/', 'il', '']\n",
    "\n",
    "# Source taken from https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' #anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "data_train = np.genfromtxt(\"trainingmix.tsv\", delimiter='\\t', dtype= str)\n",
    "\n",
    "features = []\n",
    "li=[]\n",
    "ascii=[]\n",
    "twitHandle = data_train[:,1] # this gets the tweet handles [column B]\n",
    "\n",
    "#print(twitHandle)\n",
    "#print(y)\n",
    "\n",
    "count_all = Counter()\n",
    "\n",
    "for i in range(len(twitHandle)):\n",
    "    display.clear_output(wait=True) #refresh display\n",
    "    print(i+1) # Show progress of user retrieval\n",
    "    #get user's timeline of recent tweets on # of items\n",
    "    for status in tweepy.Cursor(myApi.user_timeline, screen_name = twitHandle[i]).items(5):\n",
    "\n",
    "        \n",
    "        terms_stop = [term for term in preprocess(status.text) if term not in stop]\n",
    "            \n",
    "        # Source taken from https://marcobonzanini.com/2015/03/17/mining-twitter-data-with-python-part-3-term-frequencies/    \n",
    "        # Count hashtags only\n",
    "        terms_hash = [term for term in preprocess(status.text) \n",
    "                  if term.startswith('#')]\n",
    "        # Count terms only (no hashtags, no mentions)\n",
    "        terms_only = [term for term in preprocess(status.text) \n",
    "                  if term not in stop and\n",
    "                  not term.startswith(('#', '@', 'http'))] \n",
    "    \n",
    "        #print(\"user id :\", status.user.id)\n",
    "\n",
    "        # Update the counter\n",
    "        count_all.update(terms_only)\n",
    "        features = str(count_all.most_common(8)) # N must be even number\n",
    "        features = [term for term in preprocess(features) if term not in stop]\n",
    "        features = [term.encode('unicode_escape').decode('UTF-8') for term in features]\n",
    "        \n",
    "    #ascii = [[ord(ch) for ch in word] for word in features]\n",
    "    for word in features:\n",
    "        innerlist = []\n",
    "        for ch in word:\n",
    "            innerlist.append(ord(ch)) # ','.join somwehere here\n",
    "        ascii.append(innerlist)\n",
    "    \n",
    "    li = ascii[0::2] #obtain all words, omitting counters\n",
    "    #print (li)\n",
    "    del features[:] # Empty List for next user most freq words\n",
    "    count_all.clear() # Clear counter\n",
    "    \n",
    "    spamWriter = csv.writer(open('Trainingshttt.csv', 'a', newline=''), delimiter=',')\n",
    "    spamWriter.writerow(li)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVM Classifier full data\n",
    "# Cross Validation to fit data and get accuracy\n",
    "# find best value of C\n",
    "\n",
    "scoreSVM=[]\n",
    "accuracy=[]\n",
    "\n",
    "for i in range (1,11):\n",
    "    modelSVM = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, tol=0.0001, C=i, multi_class='ovr',\n",
    "                     fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0,\n",
    "                     random_state=None, max_iter=1000)\n",
    "    scoreMean = (cross_val_score(modelSVM, trainX, train_y, cv=2)).mean()\n",
    "    scoreSVM.append(scoreMean)\n",
    "    modelSVM.fit(trainX, train_y)\n",
    "    expected = test_y\n",
    "    predicted = modelSVM.predict(testX)\n",
    "    accuracy.append(metrics.accuracy_score(expected, predicted))\n",
    "\n",
    "for i in range (0,10):\n",
    "    print(\"Score Mean = \", scoreSVM[i])\n",
    "\n",
    "plt.xlabel('Value of C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Values of C against Accuracy Score')\n",
    "\n",
    "for i in range (0,10):\n",
    "    plt.plot(i+1,accuracy[i],'ro')\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "# fit a SVM model to the data\n",
    "modelSVM = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, tol=0.0001, C=5.0, multi_class='ovr',\n",
    "                     fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0,\n",
    "                     random_state=None, max_iter=1000)\n",
    "modelSVM.fit(trainX, train_y)\n",
    "print(modelSVM)\n",
    "# make predictions\n",
    "expected = test_y\n",
    "predicted = modelSVM.predict(testX)\n",
    "# summarize the fit of the model\n",
    "# Classification report\n",
    "print(\"Classification report \", metrics.classification_report(expected, predicted))\n",
    "\n",
    "# Confusion Metric\n",
    "confMatrix = metrics.confusion_matrix(expected, predicted)\n",
    "print(\"Confusion Metric = \", confMatrix)\n",
    "\n",
    "# Accuracy Score\n",
    "print(\"Accuracy = \", metrics.accuracy_score(expected, predicted))\n",
    "\n",
    "#True Positive Rate\n",
    "tpr = (confMatrix[0,0] / (confMatrix[1,0] + confMatrix[0,0]))\n",
    "print(\"Sensitivity = \", tpr)\n",
    "\n",
    "#True Negative Rate\n",
    "tnr = (confMatrix[1,1] / (confMatrix[0,1] + confMatrix[1,1]))\n",
    "print(\"Specificity = \", tnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
